{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sJcd86VKD7Ig"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oG594JnOD9Vp"
      },
      "outputs": [],
      "source": [
        "img_dir = '/content/images'\n",
        "images = os.listdir(img_dir)\n",
        "img_files = [os.path.join(img_dir, img) for img in images]\n",
        "len(img_files)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hK9YgGeJESPv"
      },
      "outputs": [],
      "source": [
        "def load_image(img_pth):\n",
        "  with open(img_pth, 'rb') as f:\n",
        "    image_bytes = f.read()\n",
        "  image = Image.open(BytesIO(image_bytes))\n",
        "  return image, image_bytes\n",
        "\n",
        "def show_image(image, title=''):\n",
        "  plt.imshow(image)\n",
        "  plt.axis('on')\n",
        "  plt.title(title)\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P2CZeW6aEsPL",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "show_image(load_image(img_files[0])[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5rBN-yFyLZHz"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CLIP-IQA"
      ],
      "metadata": {
        "id": "qkrqCcygXTw8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade --force-reinstall -U torchmetrics[multimodal]"
      ],
      "metadata": {
        "id": "F81dtW1n4yav",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchmetrics.functional.multimodal import clip_image_quality_assessment\n",
        "from torchvision.transforms.functional import pil_to_tensor"
      ],
      "metadata": {
        "id": "asJKagzCY2AZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scores = []\n",
        "for img_file in img_files:\n",
        "  img, _ = load_image(img_file)\n",
        "  show_image(img)\n",
        "  img_tensor = pil_to_tensor(img)\n",
        "  # options available:\n",
        "  # quality\n",
        "  # brightness\n",
        "  # noisiness\n",
        "  # colorfullness\n",
        "  # sharpness\n",
        "  # contrast\n",
        "  # complexity\n",
        "  # natural\n",
        "  # happy\n",
        "  # scary\n",
        "  # new\n",
        "  # warm\n",
        "  # real\n",
        "  # beautiful\n",
        "  # lonely\n",
        "  # relaxing\n",
        "  score = clip_image_quality_assessment(img_tensor, prompts=(\"quality\", (\"aesthetic photo\", \"unaesthetic photo\")))\n",
        "\n",
        "  aesthetic_score = score['user_defined_0'].item()\n",
        "\n",
        "  scores.append({\n",
        "      'img': img_file.split('/')[-1],\n",
        "      'aesthetic_score': aesthetic_score,\n",
        "  })\n",
        "  print('aesthetic_score:', aesthetic_score)\n",
        "\n",
        "pd.DataFrame(scores).describe()"
      ],
      "metadata": {
        "id": "_KrX6VKDYEmm",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CLIP: Classification"
      ],
      "metadata": {
        "id": "0jZmB2jk9qTd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade --force-reinstall -U salesforce-lavis"
      ],
      "metadata": {
        "id": "9Uu-d6aQZRgn",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from PIL import Image\n",
        "\n",
        "from lavis.models import load_model_and_preprocess"
      ],
      "metadata": {
        "id": "eajlg_rR9wI3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# setup device to use\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "dgdOkblc92Pr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model, vis_processors, txt_processors = load_model_and_preprocess(\"clip_feature_extractor\", model_type=\"ViT-B-16\", is_eval=True, device=device)"
      ],
      "metadata": {
        "id": "7QawQvW795ii"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cls_names = [\"Vibrant, sharp photo with inviting ambiance, unique perspective, excellent lighting, enticing food presentation, and captivating details\",\n",
        "             \"Dull, blurry photo with poor composition, harsh lighting, unappealing food presentation, and lack of engaging details\"]\n",
        "# Optional to use prompts to guide the model\n",
        "cls_names = [txt_processors[\"eval\"](cls_nm) for cls_nm in cls_names]"
      ],
      "metadata": {
        "id": "qYihWPoM9yld"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def score_image(image):\n",
        "  raw_image = image.convert(\"RGB\")\n",
        "  image = vis_processors[\"eval\"](raw_image).unsqueeze(0).to(device)\n",
        "  sample = {\"image\": image, \"text_input\": cls_names}\n",
        "\n",
        "  clip_features = model.extract_features(sample)\n",
        "\n",
        "  image_features = clip_features.image_embeds_proj\n",
        "  text_features = clip_features.text_embeds_proj\n",
        "\n",
        "  sims = (image_features @ text_features.t())[0] / 0.01\n",
        "  probs = torch.nn.Softmax(dim=0)(sims).tolist()\n",
        "\n",
        "  return probs[0]"
      ],
      "metadata": {
        "id": "QcIOz2go-I9A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scores = []\n",
        "for img_file in img_files:\n",
        "  img, _ = load_image(img_file)\n",
        "  show_image(img)\n",
        "  aesthetics_score = score_image(img)\n",
        "\n",
        "  scores.append({\n",
        "      'img': img_file.split('/')[-1],\n",
        "      'score': aesthetics_score\n",
        "  })\n",
        "  print(\"aesthetics_score:\", aesthetics_score)\n",
        "\n",
        "pd.DataFrame(scores).describe()"
      ],
      "metadata": {
        "id": "pNdOk3UM-Laz",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BLIP: Classification"
      ],
      "metadata": {
        "id": "d3HYVMsACcZx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade --force-reinstall -U salesforce-lavis"
      ],
      "metadata": {
        "id": "5MO3EnJQCg9-",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from PIL import Image\n",
        "\n",
        "from lavis.models import load_model_and_preprocess\n",
        "from lavis.processors.blip_processors import BlipCaptionProcessor"
      ],
      "metadata": {
        "id": "Rayeo2V4CjAt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# setup device to use\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "kLimkkpqClI1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model, vis_processors, _ = load_model_and_preprocess(\"blip_feature_extractor\", model_type=\"base\", is_eval=True, device=device)"
      ],
      "metadata": {
        "id": "jqlt8_EQ-7xJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cls_names = [\"Vibrant, sharp photo with inviting ambiance, unique perspective, excellent lighting, enticing food presentation, and captivating details\",\n",
        "             \"Dull, blurry photo with poor composition, harsh lighting, unappealing food presentation, and lack of engaging details\"]\n",
        "\n",
        "text_processor = BlipCaptionProcessor()\n",
        "\n",
        "cls_prompt = [text_processor(cls_nm) for cls_nm in cls_names]"
      ],
      "metadata": {
        "id": "-JkhdZDyCmyB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def score_image(image):\n",
        "  raw_image = image.convert(\"RGB\")\n",
        "  image = vis_processors[\"eval\"](raw_image).unsqueeze(0).to(device)\n",
        "  sample = {\"image\": image, \"text_input\": cls_names}\n",
        "\n",
        "  image_features = model.extract_features(sample, mode=\"image\").image_embeds_proj[:, 0]\n",
        "  text_features = model.extract_features(sample, mode=\"text\").text_embeds_proj[:, 0]\n",
        "\n",
        "  sims = (image_features @ text_features.t())[0] / model.temp\n",
        "  probs = torch.nn.Softmax(dim=0)(sims).tolist()\n",
        "\n",
        "  return probs[0]"
      ],
      "metadata": {
        "id": "kxUWfxu-Cncg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scores = []\n",
        "for img_file in img_files:\n",
        "  img, _ = load_image(img_file)\n",
        "  show_image(img)\n",
        "  aesthetics_score = score_image(img)\n",
        "\n",
        "  scores.append({\n",
        "      'img': img_file.split('/')[-1],\n",
        "      'score': aesthetics_score\n",
        "  })\n",
        "  print(\"aesthetics_score:\", aesthetics_score)\n",
        "\n",
        "pd.DataFrame(scores).describe()"
      ],
      "metadata": {
        "id": "rd8MUFsfDxCe",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "39N_D8jjECaS"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "R2tm7jobEMRO",
        "jAdzMsUvLEcI",
        "iLklcYDMo3SW",
        "qkrqCcygXTw8",
        "0jZmB2jk9qTd",
        "d3HYVMsACcZx"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}